"""
Polymarket Scraper - ê¸°ì—… ê´€ë ¨ ë§ˆì¼“ ìˆ˜ì§‘
==========================================
Polymarketì—ì„œ ê¸°ì—… ê´€ë ¨ ë§ˆì¼“ì„ ìˆ˜ì§‘í•˜ê³  í•„í„°ë§í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
from datetime import datetime
import re
from typing import List, Dict, Optional

# ì£¼ìš” ê¸°ì—… í‚¤ì›Œë“œ (í™•ì¥ ê°€ëŠ¥)
COMPANY_KEYWORDS = [
    # Tech Companies
    'openai', 'google', 'microsoft', 'apple', 'meta', 'facebook', 'amazon', 'tesla',
    'nvidia', 'netflix', 'twitter', 'x.com', 'uber', 'airbnb', 'spotify', 'snapchat',
    'tiktok', 'bytedance', 'alibaba', 'tencent', 'samsung', 'sony', 'intel', 'amd',
    'qualcomm', 'oracle', 'salesforce', 'adobe', 'paypal', 'visa', 'mastercard',
    
    # AI Companies
    'anthropic', 'claude', 'deepmind', 'stability ai', 'midjourney', 'runway',
    'character.ai', 'perplexity', 'cohere', 'mistral', 'inflection',
    
    # Crypto/Blockchain Companies
    'coinbase', 'binance', 'kraken', 'ftx', 'crypto.com', 'gemini', 'bitfinex',
    'ethereum', 'solana', 'polygon', 'avalanche', 'cardano', 'polkadot',
    
    # Automotive
    'ford', 'gm', 'general motors', 'toyota', 'honda', 'bmw', 'mercedes', 'volkswagen',
    'rivian', 'lucid', 'nio', 'xpeng', 'li auto',
    
    # Other Major Companies
    'disney', 'warner bros', 'paramount', 'comcast', 'verizon', 'at&t', 't-mobile',
    'boeing', 'lockheed', 'spacex', 'blue origin', 'virgin galactic',
    'pfizer', 'moderna', 'johnson & johnson', 'novavax',
    'walmart', 'target', 'costco', 'home depot', 'lowes',
]

# ì œí’ˆ ì¶œì‹œ ê´€ë ¨ í‚¤ì›Œë“œ
PRODUCT_KEYWORDS = [
    'release', 'launch', 'announce', 'announcement', 'product', 'model', 'version',
    'update', 'upgrade', 'feature', 'beta', 'alpha', 'preview', 'demo',
    'ship', 'shipping', 'available', 'coming', 'debut', 'unveil', 'reveal'
]

# ì •ë³´ ìš°ìœ„ê°€ ìˆì„ ìˆ˜ ìˆëŠ” íŒ¨í„´
INSIDER_INFO_PATTERNS = [
    r'release.*date',
    r'launch.*date',
    r'when.*will.*release',
    r'when.*will.*launch',
    r'date.*of.*release',
    r'date.*of.*launch',
    r'product.*release',
    r'new.*model',
    r'new.*version',
    r'upcoming.*release',
]


class PolymarketScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        })
        self.base_url = "https://polymarket.com"
        
    def fetch_markets_page(self, page: int = 1, category: Optional[str] = None) -> Optional[str]:
        """Polymarket ë§ˆì¼“ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # Polymarketì˜ ë§ˆì¼“ í˜ì´ì§€
            url = f"{self.base_url}/markets"
            params = {}
            if category:
                params['category'] = category
            if page > 1:
                params['page'] = page
            
            response = self.session.get(url, params=params, timeout=15)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"Error fetching page {page}: {e}")
            return None
    
    def parse_markets_from_html(self, html: str) -> List[Dict]:
        """HTMLì—ì„œ ë§ˆì¼“ ì •ë³´ íŒŒì‹±"""
        markets = []
        soup = BeautifulSoup(html, 'html.parser')
        
        # Polymarketì€ React ê¸°ë°˜ì´ë¯€ë¡œ, JSON-LDë‚˜ data ì†ì„±ì—ì„œ ì •ë³´ ì¶”ì¶œ ì‹œë„
        # 1. JSON-LD ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        scripts = soup.find_all('script', type='application/json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                # ì¤‘ì²©ëœ êµ¬ì¡°ì—ì„œ ë§ˆì¼“ ì •ë³´ ì°¾ê¸°
                if isinstance(data, dict):
                    # ë‹¤ì–‘í•œ ê°€ëŠ¥í•œ í‚¤ í™•ì¸
                    for key in ['events', 'markets', 'data', 'items']:
                        if key in data and isinstance(data[key], list):
                            for item in data[key]:
                                if isinstance(item, dict):
                                    title = item.get('title') or item.get('question') or item.get('name', '')
                                    if title:
                                        slug = item.get('slug', '') or item.get('id', '')
                                        link = item.get('url', '')
                                        if not link and slug:
                                            link = f"{self.base_url}/event/{slug}"
                                        markets.append({
                                            'title': title,
                                            'description': item.get('description', ''),
                                            'link': link,
                                            'scraped_at': datetime.now().isoformat()
                                        })
            except (json.JSONDecodeError, AttributeError):
                continue
        
        # 2. ì¼ë°˜ì ì¸ HTML êµ¬ì¡°ì—ì„œ ì¶”ì¶œ (í´ë°±)
        if not markets:
            # ë§í¬ê°€ /event/ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ë§í¬ ì°¾ê¸°
            event_links = soup.find_all('a', href=re.compile(r'/event/'))
            seen_titles = set()
            
            for link_elem in event_links:
                try:
                    href = link_elem.get('href', '')
                    title = link_elem.get_text(strip=True)
                    
                    # ì¤‘ë³µ ì œê±°
                    if title and title not in seen_titles:
                        seen_titles.add(title)
                        full_link = self.base_url + href if not href.startswith('http') else href
                        
                        markets.append({
                            'title': title,
                            'description': '',
                            'link': full_link,
                            'scraped_at': datetime.now().isoformat()
                        })
                except Exception:
                    continue
        
        return markets
    
    def get_category_tag_id(self, category_name: str = "tech") -> Optional[str]:
        """ì¹´í…Œê³ ë¦¬ ì´ë¦„ìœ¼ë¡œ tag_id ì°¾ê¸°"""
        try:
            tags_url = "https://gamma-api.polymarket.com/tags"
            response = self.session.get(tags_url, timeout=15)
            
            if response.status_code == 200:
                tags = response.json()
                if isinstance(tags, list):
                    for tag in tags:
                        if isinstance(tag, dict):
                            label = tag.get('label', '').lower()
                            if category_name.lower() in label or label in category_name.lower():
                                return tag.get('id')
                elif isinstance(tags, dict):
                    # ì‘ë‹µì´ dictì¸ ê²½ìš°
                    if 'data' in tags:
                        for tag in tags['data']:
                            label = tag.get('label', '').lower()
                            if category_name.lower() in label or label in category_name.lower():
                                return tag.get('id')
        except Exception as e:
            print(f"Error fetching tags: {e}")
        
        return None
    
    def fetch_markets_api(self, limit: int = 100, category: Optional[str] = None) -> List[Dict]:
        """Polymarket APIë¥¼ í†µí•´ ë§ˆì¼“ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        markets = []
        
        # ì¹´í…Œê³ ë¦¬ í•„í„°ë§ì„ ìœ„í•œ tag_id ê°€ì ¸ì˜¤ê¸°
        tag_id = None
        if category:
            tag_id = self.get_category_tag_id(category)
            if tag_id:
                print(f"âœ… '{category}' ì¹´í…Œê³ ë¦¬ tag_id: {tag_id}")
            else:
                print(f"âš ï¸  '{category}' ì¹´í…Œê³ ë¦¬ tag_idë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì „ì²´ ë§ˆì¼“ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.")
        
        # Polymarket Markets API ì‹œë„
        try:
            markets_url = "https://gamma-api.polymarket.com/markets"
            params = {
                'closed': 'false',
                'limit': limit,
                'offset': 0
            }
            
            if tag_id:
                params['tag_id'] = tag_id
            
            response = self.session.get(markets_url, params=params, timeout=15)
            
            if response.status_code == 200:
                data = response.json()
                # API ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±
                if isinstance(data, list):
                    markets = data
                elif isinstance(data, dict):
                    if 'data' in data:
                        markets = data['data']
                    elif 'events' in data:
                        markets = data['events']
                    elif 'markets' in data:
                        markets = data['markets']
                    
                    # markets ë¦¬ìŠ¤íŠ¸ ì•ˆì— dataê°€ ìˆëŠ” ê²½ìš°
                    if markets and len(markets) > 0 and isinstance(markets[0], dict) and 'data' in markets[0]:
                        markets = markets[0]['data']
                
                # ë§ˆì¼“ ë°ì´í„° ì •ê·œí™”
                normalized_markets = []
                for market in markets:
                    if isinstance(market, dict):
                        # Polymarket API ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ ì •ê·œí™”
                        title = market.get('question') or market.get('title') or market.get('name', '')
                        if title:
                            slug = market.get('slug', '') or market.get('id', '')
                            link = market.get('url', '')
                            if not link and slug:
                                link = f"{self.base_url}/event/{slug}"
                            
                            # conditionId ì¶”ì¶œ
                            condition_id = market.get('conditionId') or market.get('condition_id') or market.get('id') or ''
                            if not condition_id and slug:
                                condition_id = slug
                            
                            normalized_markets.append({
                                'title': title,
                                'question': title,  # APIì™€ ì¼ê´€ì„± ìœ ì§€
                                'description': market.get('description', ''),
                                'link': link,
                                'conditionId': condition_id,
                                'id': market.get('id') or slug or condition_id,
                                'slug': slug,
                                'outcomes': market.get('outcomes', ['Yes', 'No']),
                                'closed': market.get('closed', False),
                                'scraped_at': datetime.now().isoformat()
                            })
                markets = normalized_markets
        except Exception as e:
            print(f"API fetch failed: {e}")
        
        # API ì‹¤íŒ¨ ì‹œ ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ í´ë°±
        if not markets:
            try:
                html = self.fetch_markets_page(category=category)
                if html:
                    markets = self.parse_markets_from_html(html)
            except Exception as e:
                print(f"Web scraping also failed: {e}")
        
        return markets
    
    def is_company_related(self, title: str, description: str = "") -> tuple[bool, List[str]]:
        """ë§ˆì¼“ì´ ê¸°ì—… ê´€ë ¨ì¸ì§€ í™•ì¸"""
        text = (title + " " + description).lower()
        matched_companies = []
        
        for company in COMPANY_KEYWORDS:
            if company.lower() in text:
                matched_companies.append(company)
        
        return len(matched_companies) > 0, matched_companies
    
    def has_insider_info_potential(self, title: str, description: str = "") -> bool:
        """ë‚´ë¶€ ì •ë³´ ìš°ìœ„ê°€ ìˆì„ ìˆ˜ ìˆëŠ” ë§ˆì¼“ì¸ì§€ í™•ì¸"""
        text = (title + " " + description).lower()
        
        # ì œí’ˆ ì¶œì‹œ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
        has_product_keyword = any(keyword in text for keyword in PRODUCT_KEYWORDS)
        
        # íŒ¨í„´ ë§¤ì¹­
        has_pattern = any(re.search(pattern, text, re.I) for pattern in INSIDER_INFO_PATTERNS)
        
        return has_product_keyword or has_pattern
    
    def filter_company_markets(self, markets: List[Dict]) -> pd.DataFrame:
        """ê¸°ì—… ê´€ë ¨ ë§ˆì¼“ í•„í„°ë§"""
        filtered = []
        
        for market in markets:
            # ë‹¤ì–‘í•œ í•„ë“œëª…ì—ì„œ title ê°€ì ¸ì˜¤ê¸°
            title = market.get('title', '') or market.get('question', '') or market.get('name', '')
            description = market.get('description', '') or market.get('desc', '')
            
            is_company, companies = self.is_company_related(title, description)
            has_insider_potential = self.has_insider_info_potential(title, description)
            
            if is_company:
                # í‘œì¤€í™”ëœ í•„ë“œëª…ìœ¼ë¡œ ì €ì¥
                filtered_market = {
                    'title': title,
                    'description': description,
                    'link': market.get('link', '') or market.get('url', ''),
                    'is_company_related': True,
                    'matched_companies': ', '.join(companies),
                    'has_insider_potential': has_insider_potential,
                    'scraped_at': market.get('scraped_at', datetime.now().isoformat())
                }
                filtered.append(filtered_market)
        
        return pd.DataFrame(filtered)
    
    def scrape_all_markets(self, max_pages: int = 10, use_selenium: bool = False, category: Optional[str] = None) -> pd.DataFrame:
        """ëª¨ë“  ë§ˆì¼“ ìˆ˜ì§‘ ë° í•„í„°ë§"""
        category_text = f" ({category} ì¹´í…Œê³ ë¦¬)" if category else ""
        print(f"ğŸ” Polymarket ë§ˆì¼“ ìˆ˜ì§‘ ì¤‘{category_text}...")
        all_markets = []
        
        # Selenium ì‚¬ìš© ì˜µì…˜
        if use_selenium:
            try:
                from selenium import webdriver
                from selenium.webdriver.common.by import By
                from selenium.webdriver.support.ui import WebDriverWait
                from selenium.webdriver.support import expected_conditions as EC
                
                print("ğŸŒ Seleniumì„ ì‚¬ìš©í•˜ì—¬ ë™ì  ì½˜í…ì¸  ë¡œë“œ ì¤‘...")
                options = webdriver.ChromeOptions()
                options.add_argument('--headless')
                options.add_argument('--no-sandbox')
                options.add_argument('--disable-dev-shm-usage')
                
                driver = webdriver.Chrome(options=options)
                url = f"{self.base_url}/markets"
                if category:
                    url += f"?category={category}"
                driver.get(url)
                
                # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ì½˜í…ì¸  ë¡œë“œ
                for i in range(3):
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(2)
                
                html = driver.page_source
                driver.quit()
                
                markets = self.parse_markets_from_html(html)
                all_markets.extend(markets)
                print(f"âœ… Seleniumì„ í†µí•´ {len(markets)}ê°œ ë§ˆì¼“ ìˆ˜ì§‘")
            except ImportError:
                print("âš ï¸  Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¼ë°˜ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                use_selenium = False
            except Exception as e:
                print(f"âš ï¸  Selenium ì˜¤ë¥˜: {e}. ì¼ë°˜ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                use_selenium = False
        
        # API ë°©ì‹ ì‹œë„ (Selenium ë¯¸ì‚¬ìš© ë˜ëŠ” ì‹¤íŒ¨ ì‹œ)
        if not use_selenium or len(all_markets) == 0:
            markets = self.fetch_markets_api(limit=500, category=category)
            if markets:
                all_markets.extend(markets)
                print(f"âœ… APIë¥¼ í†µí•´ {len(markets)}ê°œ ë§ˆì¼“ ìˆ˜ì§‘")
            else:
                # ì›¹ ìŠ¤í¬ë˜í•‘ ë°©ì‹
                for page in range(1, max_pages + 1):
                    html = self.fetch_markets_page(page=page, category=category)
                    if html:
                        markets = self.parse_markets_from_html(html)
                        if not markets:
                            break
                        all_markets.extend(markets)
                        print(f"ğŸ“„ í˜ì´ì§€ {page}: {len(markets)}ê°œ ë§ˆì¼“ ìˆ˜ì§‘")
                        time.sleep(1)  # Rate limiting
                    else:
                        break
        
        # ì¤‘ë³µ ì œê±°
        seen_titles = set()
        unique_markets = []
        for market in all_markets:
            title = market.get('title', '') or market.get('question', '')
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_markets.append(market)
        
        print(f"\nğŸ“Š ì´ {len(unique_markets)}ê°œ ê³ ìœ  ë§ˆì¼“ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ê¸°ì—… ê´€ë ¨ ë§ˆì¼“ í•„í„°ë§
        print("\nğŸ” ê¸°ì—… ê´€ë ¨ ë§ˆì¼“ í•„í„°ë§ ì¤‘...")
        df = self.filter_company_markets(unique_markets)
        
        print(f"âœ… {len(df)}ê°œ ê¸°ì—… ê´€ë ¨ ë§ˆì¼“ ë°œê²¬")
        if len(df) > 0 and 'has_insider_potential' in df.columns:
            print(f"   - ë‚´ë¶€ ì •ë³´ ìš°ìœ„ ê°€ëŠ¥ì„±: {df['has_insider_potential'].sum()}ê°œ")
        
        return df
    
    def scrape_tech_markets(self, max_pages: int = 10, use_selenium: bool = False) -> pd.DataFrame:
        """Tech ì¹´í…Œê³ ë¦¬ ë§ˆì¼“ë§Œ ìˆ˜ì§‘"""
        return self.scrape_all_markets(max_pages=max_pages, use_selenium=use_selenium, category="tech")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = PolymarketScraper()
    df = scraper.scrape_all_markets(max_pages=5)
    
    if len(df) > 0:
        # ê²°ê³¼ ì €ì¥
        output_file = "polymarket_company_markets.csv"
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
        
        # ë‚´ë¶€ ì •ë³´ ìš°ìœ„ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë§ˆì¼“ë§Œ ì¶œë ¥
        insider_markets = df[df['has_insider_potential'] == True]
        if len(insider_markets) > 0:
            print("\nğŸ¯ ë‚´ë¶€ ì •ë³´ ìš°ìœ„ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë§ˆì¼“:")
            for idx, row in insider_markets.iterrows():
                print(f"\n  {row['title']}")
                print(f"    ê¸°ì—…: {row['matched_companies']}")
                print(f"    ë§í¬: {row.get('link', 'N/A')}")
    else:
        print("\nâš ï¸  ê¸°ì—… ê´€ë ¨ ë§ˆì¼“ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()

