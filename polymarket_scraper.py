"""
Polymarket Scraper - ê¸°ì—… ê´€ë ¨ ë§ˆì¼“ ìˆ˜ì§‘
==========================================
Polymarketì—ì„œ ê¸°ì—… ê´€ë ¨ ë§ˆì¼“ì„ ìˆ˜ì§‘í•˜ê³  í•„í„°ë§í•©ë‹ˆë‹¤.
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import time
from datetime import datetime
import re
from typing import List, Dict, Optional

# ì£¼ìš” ê¸°ì—… í‚¤ì›Œë“œ (í™•ì¥ ê°€ëŠ¥)
COMPANY_KEYWORDS = [
    # Tech Companies
    'openai', 'google', 'microsoft', 'apple', 'meta', 'facebook', 'amazon', 'tesla',
    'nvidia', 'netflix', 'twitter', 'x.com', 'uber', 'airbnb', 'spotify', 'snapchat',
    'tiktok', 'bytedance', 'alibaba', 'tencent', 'samsung', 'sony', 'intel', 'amd',
    'qualcomm', 'oracle', 'salesforce', 'adobe', 'paypal', 'visa', 'mastercard',
    
    # AI Companies
    'anthropic', 'claude', 'deepmind', 'stability ai', 'midjourney', 'runway',
    'character.ai', 'perplexity', 'cohere', 'mistral', 'inflection',
    
    # Crypto/Blockchain Companies
    'coinbase', 'binance', 'kraken', 'ftx', 'crypto.com', 'gemini', 'bitfinex',
    'ethereum', 'solana', 'polygon', 'avalanche', 'cardano', 'polkadot',
    
    # Automotive
    'ford', 'gm', 'general motors', 'toyota', 'honda', 'bmw', 'mercedes', 'volkswagen',
    'rivian', 'lucid', 'nio', 'xpeng', 'li auto',
    
    # Other Major Companies
    'disney', 'warner bros', 'paramount', 'comcast', 'verizon', 'at&t', 't-mobile',
    'boeing', 'lockheed', 'spacex', 'blue origin', 'virgin galactic',
    'pfizer', 'moderna', 'johnson & johnson', 'novavax',
    'walmart', 'target', 'costco', 'home depot', 'lowes',
]

# ì œí’ˆ ì¶œì‹œ ê´€ë ¨ í‚¤ì›Œë“œ
PRODUCT_KEYWORDS = [
    'release', 'launch', 'announce', 'announcement', 'product', 'model', 'version',
    'update', 'upgrade', 'feature', 'beta', 'alpha', 'preview', 'demo',
    'ship', 'shipping', 'available', 'coming', 'debut', 'unveil', 'reveal'
]

# ì •ë³´ ìš°ìœ„ê°€ ìˆì„ ìˆ˜ ìˆëŠ” íŒ¨í„´
INSIDER_INFO_PATTERNS = [
    r'release.*date',
    r'launch.*date',
    r'when.*will.*release',
    r'when.*will.*launch',
    r'date.*of.*release',
    r'date.*of.*launch',
    r'product.*release',
    r'new.*model',
    r'new.*version',
    r'upcoming.*release',
]


class PolymarketScraper:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        })
        self.base_url = "https://polymarket.com"
        
    def fetch_markets_page(self, page: int = 1, category: Optional[str] = None) -> Optional[str]:
        """Polymarket ë§ˆì¼“ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸°"""
        try:
            # Polymarketì˜ ë§ˆì¼“ í˜ì´ì§€
            url = f"{self.base_url}/markets"
            params = {}
            if category:
                params['category'] = category
            if page > 1:
                params['page'] = page
            
            response = self.session.get(url, params=params, timeout=15)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"Error fetching page {page}: {e}")
            return None
    
    def parse_markets_from_html(self, html: str) -> List[Dict]:
        """HTMLì—ì„œ ë§ˆì¼“ ì •ë³´ íŒŒì‹±"""
        markets = []
        soup = BeautifulSoup(html, 'html.parser')
        
        # Polymarketì€ React ê¸°ë°˜ì´ë¯€ë¡œ, JSON-LDë‚˜ data ì†ì„±ì—ì„œ ì •ë³´ ì¶”ì¶œ ì‹œë„
        # 1. JSON-LD ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ì—ì„œ ë°ì´í„° ì¶”ì¶œ
        scripts = soup.find_all('script', type='application/json')
        for script in scripts:
            try:
                data = json.loads(script.string)
                # ì¤‘ì²©ëœ êµ¬ì¡°ì—ì„œ ë§ˆì¼“ ì •ë³´ ì°¾ê¸°
                if isinstance(data, dict):
                    # ë‹¤ì–‘í•œ ê°€ëŠ¥í•œ í‚¤ í™•ì¸
                    for key in ['events', 'markets', 'data', 'items']:
                        if key in data and isinstance(data[key], list):
                            for item in data[key]:
                                if isinstance(item, dict):
                                    title = item.get('title') or item.get('question') or item.get('name', '')
                                    if title:
                                        markets.append({
                                            'title': title,
                                            'description': item.get('description', ''),
                                            'link': item.get('url') or item.get('slug', ''),
                                            'scraped_at': datetime.now().isoformat()
                                        })
            except (json.JSONDecodeError, AttributeError):
                continue
        
        # 2. ì¼ë°˜ì ì¸ HTML êµ¬ì¡°ì—ì„œ ì¶”ì¶œ (í´ë°±)
        if not markets:
            # ë§í¬ê°€ /event/ë¡œ ì‹œì‘í•˜ëŠ” ëª¨ë“  ë§í¬ ì°¾ê¸°
            event_links = soup.find_all('a', href=re.compile(r'/event/'))
            seen_titles = set()
            
            for link_elem in event_links:
                try:
                    href = link_elem.get('href', '')
                    title = link_elem.get_text(strip=True)
                    
                    # ì¤‘ë³µ ì œê±°
                    if title and title not in seen_titles:
                        seen_titles.add(title)
                        full_link = self.base_url + href if not href.startswith('http') else href
                        
                        markets.append({
                            'title': title,
                            'description': '',
                            'link': full_link,
                            'scraped_at': datetime.now().isoformat()
                        })
                except Exception:
                    continue
        
        return markets
    
    def fetch_markets_api(self, limit: int = 100) -> List[Dict]:
        """Polymarket APIë¥¼ í†µí•´ ë§ˆì¼“ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        markets = []
        
        # Polymarket GraphQL API ì‹œë„
        try:
            # Polymarketì˜ GraphQL ì—”ë“œí¬ì¸íŠ¸
            graphql_url = "https://gamma-api.polymarket.com/events"
            
            # ê°„ë‹¨í•œ ì¿¼ë¦¬ë¡œ ì‹œë„
            response = self.session.get(
                graphql_url,
                params={'limit': limit, 'active': 'true'},
                timeout=15
            )
            
            if response.status_code == 200:
                data = response.json()
                # API ì‘ë‹µ êµ¬ì¡°ì— ë§ê²Œ íŒŒì‹±
                if isinstance(data, list):
                    markets = data
                elif isinstance(data, dict):
                    if 'data' in data:
                        markets = data['data']
                    elif 'events' in data:
                        markets = data['events']
                    elif 'markets' in data:
                        markets = data['markets']
        except Exception as e:
            print(f"API fetch failed: {e}")
        
        # API ì‹¤íŒ¨ ì‹œ ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ í´ë°±
        if not markets:
            try:
                html = self.fetch_markets_page()
                if html:
                    markets = self.parse_markets_from_html(html)
            except Exception as e:
                print(f"Web scraping also failed: {e}")
        
        return markets
    
    def is_company_related(self, title: str, description: str = "") -> tuple[bool, List[str]]:
        """ë§ˆì¼“ì´ ê¸°ì—… ê´€ë ¨ì¸ì§€ í™•ì¸"""
        text = (title + " " + description).lower()
        matched_companies = []
        
        for company in COMPANY_KEYWORDS:
            if company.lower() in text:
                matched_companies.append(company)
        
        return len(matched_companies) > 0, matched_companies
    
    def has_insider_info_potential(self, title: str, description: str = "") -> bool:
        """ë‚´ë¶€ ì •ë³´ ìš°ìœ„ê°€ ìˆì„ ìˆ˜ ìˆëŠ” ë§ˆì¼“ì¸ì§€ í™•ì¸"""
        text = (title + " " + description).lower()
        
        # ì œí’ˆ ì¶œì‹œ ê´€ë ¨ í‚¤ì›Œë“œ í™•ì¸
        has_product_keyword = any(keyword in text for keyword in PRODUCT_KEYWORDS)
        
        # íŒ¨í„´ ë§¤ì¹­
        has_pattern = any(re.search(pattern, text, re.I) for pattern in INSIDER_INFO_PATTERNS)
        
        return has_product_keyword or has_pattern
    
    def filter_company_markets(self, markets: List[Dict]) -> pd.DataFrame:
        """ê¸°ì—… ê´€ë ¨ ë§ˆì¼“ í•„í„°ë§"""
        filtered = []
        
        for market in markets:
            title = market.get('title', '')
            description = market.get('description', '')
            
            is_company, companies = self.is_company_related(title, description)
            has_insider_potential = self.has_insider_info_potential(title, description)
            
            if is_company:
                market['is_company_related'] = True
                market['matched_companies'] = ', '.join(companies)
                market['has_insider_potential'] = has_insider_potential
                filtered.append(market)
        
        return pd.DataFrame(filtered)
    
    def scrape_all_markets(self, max_pages: int = 10, use_selenium: bool = False) -> pd.DataFrame:
        """ëª¨ë“  ë§ˆì¼“ ìˆ˜ì§‘ ë° í•„í„°ë§"""
        print("ğŸ” Polymarket ë§ˆì¼“ ìˆ˜ì§‘ ì¤‘...")
        all_markets = []
        
        # Selenium ì‚¬ìš© ì˜µì…˜
        if use_selenium:
            try:
                from selenium import webdriver
                from selenium.webdriver.common.by import By
                from selenium.webdriver.support.ui import WebDriverWait
                from selenium.webdriver.support import expected_conditions as EC
                
                print("ğŸŒ Seleniumì„ ì‚¬ìš©í•˜ì—¬ ë™ì  ì½˜í…ì¸  ë¡œë“œ ì¤‘...")
                options = webdriver.ChromeOptions()
                options.add_argument('--headless')
                options.add_argument('--no-sandbox')
                options.add_argument('--disable-dev-shm-usage')
                
                driver = webdriver.Chrome(options=options)
                driver.get(f"{self.base_url}/markets")
                
                # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸°
                WebDriverWait(driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                
                # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ì½˜í…ì¸  ë¡œë“œ
                for i in range(3):
                    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(2)
                
                html = driver.page_source
                driver.quit()
                
                markets = self.parse_markets_from_html(html)
                all_markets.extend(markets)
                print(f"âœ… Seleniumì„ í†µí•´ {len(markets)}ê°œ ë§ˆì¼“ ìˆ˜ì§‘")
            except ImportError:
                print("âš ï¸  Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¼ë°˜ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                use_selenium = False
            except Exception as e:
                print(f"âš ï¸  Selenium ì˜¤ë¥˜: {e}. ì¼ë°˜ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                use_selenium = False
        
        # API ë°©ì‹ ì‹œë„ (Selenium ë¯¸ì‚¬ìš© ë˜ëŠ” ì‹¤íŒ¨ ì‹œ)
        if not use_selenium or len(all_markets) == 0:
            markets = self.fetch_markets_api(limit=500)
            if markets:
                all_markets.extend(markets)
                print(f"âœ… APIë¥¼ í†µí•´ {len(markets)}ê°œ ë§ˆì¼“ ìˆ˜ì§‘")
            else:
                # ì›¹ ìŠ¤í¬ë˜í•‘ ë°©ì‹
                for page in range(1, max_pages + 1):
                    html = self.fetch_markets_page(page=page)
                    if html:
                        markets = self.parse_markets_from_html(html)
                        if not markets:
                            break
                        all_markets.extend(markets)
                        print(f"ğŸ“„ í˜ì´ì§€ {page}: {len(markets)}ê°œ ë§ˆì¼“ ìˆ˜ì§‘")
                        time.sleep(1)  # Rate limiting
                    else:
                        break
        
        # ì¤‘ë³µ ì œê±°
        seen_titles = set()
        unique_markets = []
        for market in all_markets:
            title = market.get('title', '')
            if title and title not in seen_titles:
                seen_titles.add(title)
                unique_markets.append(market)
        
        print(f"\nğŸ“Š ì´ {len(unique_markets)}ê°œ ê³ ìœ  ë§ˆì¼“ ìˆ˜ì§‘ ì™„ë£Œ")
        
        # ê¸°ì—… ê´€ë ¨ ë§ˆì¼“ í•„í„°ë§
        print("\nğŸ” ê¸°ì—… ê´€ë ¨ ë§ˆì¼“ í•„í„°ë§ ì¤‘...")
        df = self.filter_company_markets(unique_markets)
        
        print(f"âœ… {len(df)}ê°œ ê¸°ì—… ê´€ë ¨ ë§ˆì¼“ ë°œê²¬")
        if len(df) > 0 and 'has_insider_potential' in df.columns:
            print(f"   - ë‚´ë¶€ ì •ë³´ ìš°ìœ„ ê°€ëŠ¥ì„±: {df['has_insider_potential'].sum()}ê°œ")
        
        return df


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    scraper = PolymarketScraper()
    df = scraper.scrape_all_markets(max_pages=5)
    
    if len(df) > 0:
        # ê²°ê³¼ ì €ì¥
        output_file = "polymarket_company_markets.csv"
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_file}")
        
        # ë‚´ë¶€ ì •ë³´ ìš°ìœ„ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë§ˆì¼“ë§Œ ì¶œë ¥
        insider_markets = df[df['has_insider_potential'] == True]
        if len(insider_markets) > 0:
            print("\nğŸ¯ ë‚´ë¶€ ì •ë³´ ìš°ìœ„ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë§ˆì¼“:")
            for idx, row in insider_markets.iterrows():
                print(f"\n  {row['title']}")
                print(f"    ê¸°ì—…: {row['matched_companies']}")
                print(f"    ë§í¬: {row.get('link', 'N/A')}")
    else:
        print("\nâš ï¸  ê¸°ì—… ê´€ë ¨ ë§ˆì¼“ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    main()

